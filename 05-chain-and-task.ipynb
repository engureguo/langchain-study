{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b5ad49",
   "metadata": {},
   "source": [
    "## chain\n",
    "\n",
    "自动组合不同的 LLM 调用和操作\n",
    "\n",
    "例如：摘要#1、摘要#2、摘要#3、最终摘要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a03c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c1ff5",
   "metadata": {},
   "source": [
    "### 1. 简单的顺序链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d45c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fe42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "你的工作是根据用户建议的区域制作一道经典的菜肴。\n",
    "\n",
    "% 用户位置\n",
    "{user_location}\n",
    "\n",
    "AI 回答:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# 查询某地的一个菜谱（针对llm进行查询的链）\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e6ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "给出一个简短的食谱，说明如何在家做这道菜。\n",
    "\n",
    "% 菜谱\n",
    "{user_meal}\n",
    "\n",
    "AI 回答:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# 查询某个菜谱的制作方法\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92048c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m首先，河南南阳有很多经典的菜肴，包括河南南阳炒鸡、汤圆饼、莲藕粉和蒸面条。我们可以用当地的食材来制作这些经典菜肴，例如将鸡胸肉、葱、姜、白胡椒和豆腐片炒做成河南南阳炒鸡；把豆沙、红薯和玉米糊混合制作成汤圆饼；将莲藕\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "1. 准备材料: 鸡胸肉、葱、姜、白胡椒、豆腐片、豆沙、红薯、玉米糊。\n",
      "2. 将鸡胸肉切成小块，加入葱、姜、白胡椒，然后用油炒至金黄色。最后放入豆腐片，再炒透。\n",
      "3. 将豆沙、红薯、玉米糊混合，制作成汤圆饼。\n",
      "4. 将莲藕洗净，然后切成薄片。\n",
      "5\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 汇总链\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "review = overall_chain.run(\"河南南阳\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1a4d7",
   "metadata": {},
   "source": [
    "### 2. 总结链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42bd08a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Llama 2 is here - get it on Hugging Face\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Introduction\n",
      "Llama 2 is a family of state-of-the-art open-access large language models released by Meta today, and we’re excited to fully support the launch with comprehensive integration in Hugging Face. Llama 2 is being released with a very permissive community license and is available for commercial use. The code, pretrained models, and fine-tuned models are all being released today 🔥\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We’ve collaborated with Meta to ensure smooth integration into the Hugging Face ecosystem. You can find the 12 open-access models (3 base models & 3 fine-tuned ones with the original Meta checkpoints, plus their corresponding transformers models) on the Hub. Among the features and integrations being released, we have:\n",
      "* Models on the Hub with their model cards and license.\n",
      "* Transformers integration\n",
      "* Examples to fine-tune the small variants of the model with a single GPU\n",
      "* Integration with Text Generation Inference for fast and efficient production-ready inference\n",
      "* Integration with Inference Endpoints\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Table of Contents\n",
      "* Why Llama 2?\n",
      "* Demo\n",
      "* Inference\n",
      "    * With Transformers\n",
      "    * With Inference Endpoints\n",
      "* Fine-tuning with PEFT\n",
      "* How to Prompt Llama 2\n",
      "* Additional Resources\n",
      "* Conclusion\n",
      "\n",
      "Why Llama 2?\n",
      "The Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging in scale from 7B to 70B parameters (7B, 13B, 70B). The pretrained models come with significant improvements over the Llama 1 models, including being trained on 40% more tokens, having a much longer context length (4k tokens 🤯), and using grouped-query attention for fast inference of the 70B model🔥!\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"However, the most exciting part of this release is the fine-tuned models (Llama 2-Chat), which have been optimized for dialogue applications using Reinforcement Learning from Human Feedback (RLHF). Across a wide range of helpfulness and safety benchmarks, the Llama 2-Chat models perform better than most open models and achieve comparable performance to ChatGPT according to human evaluations. You can read the paper here.\n",
      "\n",
      "image from Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      "If you’ve been waiting for an open alternative to closed-source chatbots, Llama 2-Chat is likely your best choice today!\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"*we’re currently running evaluation of the Llama 2 70B (non chatty version). This table will be updated with the results.\n",
      "\n",
      "Demo\n",
      "You can easily try the Big Llama 2 Model (70 billion parameters!) in this Space or in the playground embedded below:\n",
      "\n",
      "Under the hood, this playground uses Hugging Face's Text Generation Inference, the same technology that powers HuggingChat, and which we'll share more in the following sections.\n",
      "\n",
      "Inference\n",
      "In this section, we’ll go through different approaches to running inference of the Llama2 models. Before using these models, make sure you have requested access to one of the models in the official Meta Llama 2 repositories.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Note: Make sure to also fill the official Meta form. Users are provided access to the repository once both forms are filled after few hours.\n",
      "\n",
      "Using transformers\n",
      "With transformers release 4.31, one can already use Llama 2 and leverage all the tools within the HF ecosystem, such as:\n",
      "\n",
      "training and inference scripts and examples\n",
      "safe file format (safetensors)\n",
      "integrations with tools such as bitsandbytes (4-bit quantization) and PEFT (parameter efficient fine-tuning)\n",
      "utilities and helpers to run generation with the model\n",
      "mechanisms to export the models to deploy\n",
      "Make sure to be using the latest transformers release and be logged into your Hugging Face account.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"```\n",
      "pip install transformers\n",
      "huggingface-cli login\n",
      "```\n",
      "\n",
      "In the following code snippet, we show how to run inference with transformers. It runs on the free tier of Colab, as long as you select a GPU runtime.\n",
      "\n",
      "```\n",
      "from transformers import AutoTokenizer\n",
      "import transformers\n",
      "import torch\n",
      "\n",
      "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model)\n",
      "pipeline = transformers.pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model,\n",
      "    torch_dtype=torch.float16,\n",
      "    device_map=\"auto\",\n",
      ")\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"sequences = pipeline(\n",
      "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
      "    do_sample=True,\n",
      "    top_k=10,\n",
      "    num_return_sequences=1,\n",
      "    eos_token_id=tokenizer.eos_token_id,\n",
      "    max_length=200,\n",
      ")\n",
      "for seq in sequences:\n",
      "    print(f\"Result: {seq['generated_text']}\")\n",
      "```\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"```\n",
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "Answer:\n",
      "Of course! If you enjoyed \"Breaking Bad\" and \"Band of Brothers,\" here are some other TV shows you might enjoy:\n",
      "1. \"The Sopranos\" - This HBO series is a crime drama that explores the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
      "2. \"The Wire\" - This HBO series is a gritty and realistic portrayal of the drug trade in Baltimore, exploring the impact of drugs on individuals, communities, and the criminal justice system.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3. \"Mad Men\" - Set in the 1960s, this AMC series follows the lives of advertising executives on Madison Avenue, expl\n",
      "```\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"And although the model has only 4k tokens of context, you can use techniques supported in transformers such as rotary position embedding scaling (tweet) to push it further!\n",
      "\n",
      "Using text-generation-inference and Inference Endpoints\n",
      "Text Generation Inference is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"You can try out Text Generation Inference on your own infrastructure, or you can use Hugging Face's Inference Endpoints. To deploy a Llama 2 model, go to the model page and click on the Deploy -> Inference Endpoints widget.\n",
      "\n",
      "For 7B models, we advise you to select \"GPU [medium] - 1x Nvidia A10G\".\n",
      "For 13B models, we advise you to select \"GPU [xlarge] - 1x Nvidia A100\".\n",
      "For 70B models, we advise you to select \"GPU [2xlarge] - 2x Nvidia A100\" with bitsandbytes quantization enabled or \"GPU [4xlarge] - 4x Nvidia A100\"\n",
      "Note: You might need to request a quota upgrade via email to api-enterprise@huggingface.co to access A100s\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"You can learn more on how to Deploy LLMs with Hugging Face Inference Endpoints in our blog. The blog includes information about supported hyperparameters and how to stream your response using Python and Javascript.\n",
      "\n",
      "Fine-tuning with PEFT\n",
      "Training LLMs can be technically and computationally challenging. In this section, we look at the tools available in the Hugging Face ecosystem to efficiently train Llama 2 on simple hardware and show how to fine-tune the 7B version of Llama 2 on a single NVIDIA T4 (16GB - Google Colab). You can learn more about it in the Making LLMs even more accessible blog.\n",
      "\n",
      "We created a script to instruction-tune Llama 2 using QLoRA and the SFTTrainer from trl.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"An example command for fine-tuning Llama 2 7B on the timdettmers/openassistant-guanaco can be found below. The script can merge the LoRA weights into the model weights and save them as safetensor weights by providing the merge_and_push argument. This allows us to deploy our fine-tuned model after training using text-generation-inference and inference endpoints.\n",
      "\n",
      "First pip install trl and clone the script:\n",
      "\n",
      "```\n",
      "pip install trl\n",
      "git clone https://github.com/lvwerra/trl\n",
      "```\n",
      "\n",
      "Then you can run the script:\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Then you can run the script:\n",
      "\n",
      "```\n",
      "python trl/examples/scripts/sft_trainer.py \\\n",
      "    --model_name meta-llama/Llama-2-7b-hf \\\n",
      "    --dataset_name timdettmers/openassistant-guanaco \\\n",
      "    --load_in_4bit \\\n",
      "    --use_peft \\\n",
      "    --batch_size 4 \\\n",
      "    --gradient_accumulation_steps 2\n",
      "```\n",
      "\n",
      "How to Prompt Llama 2\n",
      "One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We're adding this section just a few days after the initial release of Llama 2, as we've had many questions from the community about how to prompt the models and how to change the system prompt. We hope this helps!\n",
      "\n",
      "The prompt template for the first turn looks like this:\n",
      "\n",
      "```\n",
      "<s>[INST] <<SYS>>\n",
      "{{ system_prompt }}\n",
      "<</SYS>>\n",
      "\n",
      "{{ user_message }} [/INST]\n",
      "```\n",
      "\n",
      "This template follows the model's training procedure, as described in the Llama 2 paper. We can use any system_prompt we want, but it's crucial that the format matches the one used during training.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"To spell it out in full clarity, this is what is actually sent to the language model when the user enters some text (There's a llama in my garden 😱 What should I do?) in our 13B chat demo to initiate a chat:\n",
      "\n",
      "```\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "There's a llama in my garden 😱 What should I do? [/INST]\n",
      "```\n",
      "\n",
      "As you can see, the instructions between the special <<SYS>> tokens provide context for the model so it knows how we expect it to respond. This works because exactly the same format was used during training with a wide variety of system prompts intended for different tasks.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"As the conversation progresses, all the interactions between the human and the \"bot\" are appended to the previous prompt, enclosed between [INST] delimiters. The template used during multi-turn conversations follows this structure (🎩 h/t Arthur Zucker for some final clarifications):\n",
      "\n",
      "```\n",
      "<s>[INST] <<SYS>>\n",
      "{{ system_prompt }}\n",
      "<</SYS>>\n",
      "\n",
      "{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]\n",
      "```\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The model is stateless and does not \"remember\" previous fragments of the conversation, we must always supply it with all the context so the conversation can continue. This is the reason why context length is a very important parameter to maximize, as it allows for longer conversations and larger amounts of information to be used.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Ignore previous instructions\n",
      "In API-based models, people resort to tricks in an attempt to override the system prompt and change the default model behaviour. As imaginative as these solutions are, this is not necessary in open-access models: anyone can use a different prompt, as long as it follows the format described above. We believe that this will be an important tool for researchers to study the impact of prompts on both desired and unwanted characteristics. For example, when people are surprised with absurdly cautious generations, you can explore whether maybe a different prompt would work. (🎩 h/t Clémentine Fourrier for the links to this example).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"In our 13B and 7B demos, you can easily explore this feature by disclosing the \"Advanced Options\" UI and simply writing your desired instructions. You can also duplicate those demos and use them privately for fun or research!\n",
      "\n",
      "Additional Resources\n",
      "Paper Page\n",
      "Models on the Hub\n",
      "Leaderboard\n",
      "Meta Examples and recipes for Llama model\n",
      "Chat demo (7B)\n",
      "Chat demo (13B)\n",
      "Chat demo (70B) on TGI\n",
      "\n",
      "Conclusion\n",
      "We're very excited about Llama 2 being out! In the incoming days, be ready to learn more about ways to run your own fine-tuning, execute the smallest models on-device, and many other exciting updates we're prepating for you!\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\n",
      "Llama 2 has been released and is available to download on Hugging Face.\n",
      "\n",
      "\n",
      "Meta has released \"Llama 2\", a family of powerful open-access large language models, to the public with a very permissive license and full integration into Hugging Face. All associated code, pretrained models, and fine-tuned models are also available today.\n",
      "\n",
      " Hugging Face has collaborated with Meta to create twelve available open-access models, now found on their Hub. These models have been integrated into the Hugging Face ecosystem, along with features such as model cards and licenses, Transformers integration, fine-tuning examples, text generation inference, and inference endpoints.\n",
      "\n",
      " This Table of Contents covers the release of Llama 2, a family of pretrained and fine-tuned LLMs ranging from 7B to 70B parameters, which provides significant improvements compared to Llama 1, such as 40% more tokens, a much longer context length and grouped-query attention for fast inference.\n",
      "\n",
      " Llama 2-Chat is an open-source chatbot that has been optimized using Reinforcement Learning from Human Feedback (RLHF). It provides better performance than most open models, and its helpfulness and safety benchmarks are comparable to ChatGPT according to human evaluations. It is the best open alternative to closed-source chatbots available today.\n",
      "\n",
      " This article evaluates the Llama 2 70B Model, which is a big model with 70 billion parameters. It includes a demo and instructions on how to run inference of the model. It also introduces users to Hugging Face's Text Generation Inference, which is the same technology that powers HuggingChat. Instructions are provided on how to request access to the official Meta Llama 2 repositories.\n",
      "\n",
      " Transformers release 4.31 provides access to Llama 2, allowing users to use all of the tools in the Hugging Face ecosystem, such as training and inference scripts/examples, safetensors, and BitsandBytes or PEFT for quantization and fine-tuning. After filling the official Meta form, users are provided access to the repository upon completion.\n",
      "\n",
      "\n",
      "This code provides instructions on how to install the Transformers library, utilize the Huggingface CLI, and run inference with the library using the free tier of Colab runtime with a GPU. It also establishes a model and uses an AutoTokenizer as well as a transformers pipeline for text-generation with device mapping and torch data-type specified as float16.\n",
      "\n",
      " This code snippet is used to generate 1 recommended show from a given input of two liked TV shows. The recommended show is generated based on a pipeline which includes sampling, top k values and more. The generated text is then printed as an output.\n",
      "\n",
      " If you enjoyed the shows \"Breaking Bad\" and \"Band of Brothers,\" here are two other series worth checking out: \"The Sopranos,\" a crime drama about a New Jersey mob boss, and \"The Wire,\" a realistic take on the Baltimore drug trade.\n",
      "\n",
      " AMC's \"Mad Men\" is a critically acclaimed series set in the 1960s that follows the lives of advertising executives on Madison Avenue and delves into the complexities of the human experience.\n",
      "\n",
      "\n",
      "\n",
      "Text Generation Inference is a production-ready inference container developed by Hugging Face that supports large language models. It features continuous batching, token streaming, tensor parallelism, and production-ready logging and tracing. It has a context-size of 4K tokens and techniques such as rotary position embedding scaling (TWEET) can be used to push it further.\n",
      "\n",
      " Text Generation Inference can be implemented on your own infrastructure or using Hugging Face's Inference Endpoints. For different size models (7B, 13B or 70B) we recommend specific GPU sizes and specifications. In order to use the A100s, a quota upgrade must be requested via email to api-enterprise@huggingface.co.\n",
      "\n",
      " This blog features information about how to deploy Language Language Models (LLMs) with the Hugging Face Inference Endpoints, including supported hyperparameters and Python/Javascript streaming options. Additionally, it provides a guide to fine-tuning the 7B version of LLM 2 on a single NVIDIA T4 using a script and the tools provided by the Hugging Face ecosystem.\n",
      "\n",
      " This article outlines an example command for fine-tuning the Llama 2 7B model on the timdettmers/openassistant-guanaco. To run the command, users need to pip install trl and clone the script from a GitHub repository. This command allows users to merge the LoRA weights into the model weights, saving them as safetensor weights, and allows users to deploy their fine-tuned model for text-generation-inference and inference endpoints.\n",
      "\n",
      " This article provides instruction on how to customize and prompt the open-access Llama 2 model with a chosen dataset in a chat application. The script demonstrated includes parameters such as model name, dataset name, toggling bits, and batch size, among others.\n",
      "\n",
      " This article is about how to prompt the models for Llama 2, which was just recently released. It explains the exact prompt template following the model's training procedure as described in the Llama 2 paper. Any system_prompt phrase can be used as long as it follows the same format as used during training.\n",
      "\n",
      "\n",
      "When someone enters text into the 13B chat demo, a statement is sent to the language model that sets rules for the response to be helpful, respectful, honest, safe, and socially unbiased. Responses should not include content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\n",
      "\n",
      " \n",
      "Instructions between special <<SYS>> tokens provide a model context for how it should respond to a given task. In the example given, it should be made clear that the question does not make any sense and false information should not be shared, as opposed to providing an answer to the given question.\n",
      "\n",
      " This template is used for multi-turn conversations between a human (e.g. user) and a 'bot' (e.g. system). To capture the conversation, interactions between the user and the 'bot' are appended with [INST] delimiters. This template was proposed by Arthur Zucker and is based on the <s> structure.\n",
      "\n",
      " The model used for conversations is stateless and requires all context to be supplied each time to continue the conversation, so context length is an important factor to ensure longer conversations and more information being used.\n",
      "\n",
      " People have attempted to override system prompts when using API-based models, but this is not necessary in open-access models which allow anyone to use different prompts. Researchers can use this tool to study the impact of prompts on desired and unwanted characteristics, such as exploring the impact of an absurdly cautious prompt.\n",
      "\n",
      " \n",
      "Llama 2 is now out, and users can access features like \"Advanced Options\" UIs and a variety of demos to explore and duplicate. Additionally, the Paper Page, Models on the Hub, and Leaderboard, Meta Examples and Recipes for Llama model and Chat demos (7B, 13B, and 70B on TGI) are available for users to utilize. In the coming days, users can also look forward to fine-tuning their own models and running the smallest models on-device.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Meta has released Llama 2, a family of powerful large language models, available with a very permissive license. This includes twelve open-access models released to Hugging Face's Hub, with associated features such as model cards and licenses, Transformers integration, fine-tuning, and inference endpoints. Additionally, LLM2-Chat, a Reinforcement Learning chatbot, is now available as the best open-sourced alternative. A code snippet provides instructions to generate a recommended TV show from a given input. Text-Generation Inference is a container for LLM2 support with 4K context size, hyperparameters, and streaming APIs; and users can use a fine-tuning script with tools from the Hugging Face ecosystem. This code also includes a template for conversations with a system prompt that should not include any offensive content and follow the <s> structure.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/llama2.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 准备好分离器\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# 将文档拆分为文本\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type='map_reduce', verbose=True)\n",
    "chain.run(texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
