{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a2732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee844cc",
   "metadata": {},
   "source": [
    "## 1. 对话缓冲内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ee8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e9f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc46a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Eugene.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Eugene! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Eugene.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da0c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene.\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d4ac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene.\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Eugene.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e346cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Eugene.\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Eugene.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4eadbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Eugene.\\nAI: Hello Eugene! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Eugene.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ac38",
   "metadata": {},
   "source": [
    "**demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198137df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "mem1 = ConversationBufferMemory()\n",
    "\n",
    "mem1.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "print(mem1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d365096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05bd2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem1.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d31beca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb72e8a",
   "metadata": {},
   "source": [
    "***SUMMARY***\n",
    "\n",
    "1. LLMs are 'stateless'\n",
    "2. Each transaction is independent\n",
    "3. Chatbos appear to have memory by providing *the full conversation* as 'context'\n",
    "\n",
    "But with OpenAI's **limit on the number of prompt tokens**, this approach doesn't seem to work when having a long conversation.\n",
    "\n",
    "**Langchain provides serveral kinds of 'memory' to store and accumulate the conversation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9c2bd",
   "metadata": {},
   "source": [
    "## 2. 对话缓冲窗口内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9bac6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f43b68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k: num of last conversations to save in memory\n",
    "window_memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649702b3",
   "metadata": {},
   "source": [
    "**demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0ffbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "window_memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb155e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buffered conversations in 'window memory'\n",
    "window_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8c9e0",
   "metadata": {},
   "source": [
    "**demo ends.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f53e9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation2 = ConversationChain(llm=chat, memory=window_memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e0d6b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Eugene.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Eugene! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"Hi, my name is Eugene.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddb562c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Eugene.\n",
      "AI: Hello Eugene! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10f9a279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8da08a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: What is my name?\\nAI: I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894baf01",
   "metadata": {},
   "source": [
    "## 3. 对话Token缓冲区内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d78a4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d81673b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_token_limit: num of last conversation tokens to save in memory\n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8de8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_memory.save_context({\"input\": \"AI is what?\"}, {\"output\": \"Amazing\"})\n",
    "token_memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "token_memory.save_context({\"input\": \"Chatbos are what?\"}, {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78cd4c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbos are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c817298",
   "metadata": {},
   "source": [
    "`max_token_limit`: ***num of last conversation tokens*** to save in memory.\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "max_token_limit  -->  buffered conversation string\n",
    "50  -->  AI: Amazing\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbos are what?\\nAI: Charming!\n",
    "30  -->  AI: Beautiful!\\nHuman: Chatbos are what?\\nAI: Charming!\n",
    "20  ->>  AI: Charming!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f40f8",
   "metadata": {},
   "source": [
    "## 4. 对话总结缓冲内存\n",
    "\n",
    "***Use LLM to write summary of conversations so far and keep it in memory.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f73a8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    " from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5053a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2ba1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n",
    "\n",
    "# 'create conversations'\n",
    "summary_memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "summary_memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "summary_memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0580baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings. The human mentions that they are not doing much. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and having lunch with a customer to discuss the latest in AI. The AI also reminds the human to bring their laptop to show a demo.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e8b5b",
   "metadata": {},
   "source": [
    "some examples when `max_token_limit` changes:\n",
    "\n",
    "***max_token_limit = 400***   (when 400 tokens is big enough to contain all conversations)\n",
    "\n",
    "Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
    "\n",
    "***max_token_limit = 100***\n",
    "\n",
    "System: The human and AI exchange greetings. The human mentions that they are not doing much. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and having lunch with a customer to discuss the latest in AI. The AI also reminds the human to bring their laptop to show a demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6134def2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human mentions that they are not doing much. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and having lunch with a customer to discuss the latest in AI. The AI also reminds the human to bring their laptop to show a demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A good demo to show would be a live demonstration of the LangChain project. You can showcase the features and functionality of the language translation software, highlighting its accuracy and efficiency. Additionally, you could also demonstrate any recent updates or improvements made to the project. This would give the customer a firsthand experience of the capabilities of the AI technology and its potential benefits for their business.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation4 = ConversationChain(llm=chat, memory=summary_memory, verbose=True)\n",
    "\n",
    "conversation4.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56a37b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings. The human mentions that they are not doing much. The AI informs the human about their schedule for the day, including a meeting with the product team, working on the LangChain project, and having lunch with a customer to discuss the latest in AI. The AI also reminds the human to bring their laptop to show a demo.\\nHuman: What would be a good demo to show?\\nAI: A good demo to show would be a live demonstration of the LangChain project. You can showcase the features and functionality of the language translation software, highlighting its accuracy and efficiency. Additionally, you could also demonstrate any recent updates or improvements made to the project. This would give the customer a firsthand experience of the capabilities of the AI technology and its potential benefits for their business.'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7cc2b8",
   "metadata": {},
   "source": [
    "## 5. 总结\n",
    "\n",
    "***ConversationBufferMemory***\n",
    "\n",
    "This memory allows for storing of messages and then extracts the messages in a variable.\n",
    "\n",
    "***ConversationBufferWindowMemory***\n",
    "\n",
    "This memory keeps a list of the interactions of the conversation over time. It only uses the last K interactions.\n",
    "\n",
    "***ConversationTokenBufferMemory***\n",
    "\n",
    "This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.\n",
    "\n",
    "***ConversationSummaryMemory***\n",
    "\n",
    "This memory creates a summary of the conversation over time.\n",
    "\n",
    "---\n",
    "\n",
    "Langchain also provides other types of memory:\n",
    "\n",
    "***Vector data memory***\n",
    "\n",
    "stores text(from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of text.\n",
    "\n",
    "***Entity memory***\n",
    "\n",
    "using a LLM, it remembers details about specific entities.\n",
    "\n",
    "---\n",
    "\n",
    "You can alse use multiple memories at one time. E.g., Conversation Memory + Entity Memory, to recall individuals\n",
    "\n",
    "You can also store the conversation in a conventional database, such as KV store or SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10043da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
